
\onehalfspacing
\newpage
\chapter*{Appendices - Source Code}

\section*{functions.py}

\begin{lstlisting}[language=Python]
import h5py
import numpy as np
import copy
import pandas as pd
import random
import pickle as pkl
import mobilenetv2
import tensorflow as tf
import keras
from keras.optimizers import SGD, Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint

def encode_event(array, i):
    """encode_event
    Function that takes in an hdf5 array and an index gives the run, subrun, evt, subevt, cycle of an event
    and returnsthem as a string for quick lookups
    # Arguments
        array: branch from the hdf5 files
        i: index within the array
    # Returns
	string containing run, subrun, evt, subevt, cycle
    """
    run = array['run'][i]
    subrun = array['subrun'][i]
    evt = array['evt'][i]
    subevt = array['subevt'][i]
    cycle = array['cycle'][i]
    try:
        genweight = array['genweight'][i]
        arr_str = ('{},{},{},{},{},{}'.format(run, subrun, evt, subevt, cycle, genweight))
    except KeyError:
        arr_str = ('{},{},{},{},{}'.format(run, subrun, evt, subevt, cycle))
    return arr_str



def get_files(path):
    """get_files
    Function creates a list of all the hdf5 files in a directorty
    # Arguments
        path: the path to the direectory
    # Returns
	filenames: list of filenames"""
    folder = os.listdir(os.fsencode(path))
    filenames = [os.fsdecode(file) for file in folder if os.fsdecode(file).endswith(('.h5'))]

    return filenames



def data(file):
    """data
    Function that extracts the data from the hdf5 file and returns the relevant data
    # Arguments
        file: hdf5 file name
    # Returns
	f - the hfd5 file
        file - hfd5 file name
        Train_Params - a dictionary with keys = [run, subrun, evt, subevt, cycle] and
        values = [train array index, iscc value, pdg value]
        maps - the arrays containing the image data
        cvnmaps - the branch of the hdf5 file that contains the maps
    """
    #import file
    f = h5py.File(file, 'r')

    # cvn training data branch and truth branch keys
    cvnmaps = f['rec.training.cvnmaps']
    mc = f['rec.mc.nu']
    train_event = [encode_event(cvnmaps, i) for i in range(len(cvnmaps['evt']))]
    mc_event = [encode_event(mc, i) for i in range(len(mc['evt']))]

    #mapping
    Train_Params = {}
    for i in range(len(mc_event)):
        for j in range(len(train_event)):
            #only including the run, subrun, evt, subevt, cycle data from the mc branch in the matching
            mc_data  = ','.join(mc_event[i].split(',')[:-1])
            train = train_event[j]
            if mc_data == train:
                Train_Params[mc_event[i]] = j, mc['iscc'][i][0], mc['pdg'][i][0]

    return f, file, Train_Params



def mu_cuts(f, file):
    """
    Function that takes in an hdf5 file and creates a list of events that do not pass the nu mu cuts:
    nu mu cuts defined - "CAFAna/Cuts/NumuCuts2018.h"
        cuts used - kNumuQuality && kNumuContainFD2017
    """
    files_list = [os.fsdecode(file) for file in folder if os.fsdecode(file).endswith(('.h5'))]
    cut_arr_mu = []

    ### kNumuQuality cut ###
    en_numu = f['rec.energy.numu']
    for i in range(len(en_numu['trkccE'])):
        if en_numu['trkccE'][i]<=0:
            s = encode_event(en_numu, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    cut = [encode_event(sel_remid, i) for i in len(en_numu['trkccE']) if en_numu['trkccE'][i]<=0]
    s = encode_event(cut, i)
    if s not in cut_arr_mu:
        cut_arr_mu.append(s)

    sel_remid = f['rec.sel.remid']
    for i in range(len(sel_remid['pid'])):
        if sel_remid['pid'][i]<=0:
            s = encode_event(sel_remid, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    slc = f['rec.slc']
    for i in range(len(slc['nhit'])):
        if slc['nhit'][i]<=20 or slc['ncontplanes'][i]<=4:
            s = encode_event(slc, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    cosmic = f['rec.trk.cosmic']
    for i in range(len(cosmic['ntracks'])):
        if cosmic['ntracks'][i]<=0 :
            s = encode_event(cosmic, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    ### kNumuContainFD2017 cut ###
    shwlid = f['rec.vtx.elastic.fuzzyk.png.shwlid']
    for i in range(f['rec.vtx.elastic.fuzzyk.png.shwlid']['start.x'].shape[1]):
        a = min(shwlid['start.x'][i],shwlid['stop.x'][i])
        b = max(shwlid['start.x'][i],shwlid['stop.x'][i])
        c = min(shwlid['start.y'][i], shwlid['stop.y'][i])
        d = max(shwlid['start.y'][i], shwlid['stop.y'][i])
        e = min(shwlid['start.z'][i], shwlid['stop.z'][i])
        f = max(shwlid['start.z'][i], shwlid['stop.z'][i])
        if a <=-180 or b >=180 or c <=-180 or d >=180 or e <=20 or f >=1525:
            s = encode_event(shwlid, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    f = h5py.File(file, 'r')
    kal_track = f['rec.trk.kalman.tracks']
    for i in range(len(kal_track['start.z'])):
        if kal_track['start.z'][i]>1275 or kal_track['stop.z'][i]>1275:
            s = encode_event(kal_track, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    slc = f['rec.slc']
    for i in range(len(slc['firstplane'])):
        if slc['firstplane'][i]<=1 or slc['lastplane'][i]==212 or slc['lastplane'][i]==213:
            s = encode_event(slc, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    sel = f['rec.sel.contain']
    for i in range(len(sel['kalfwdcellnd'])):
        if sel['kalyposattrans'][i]>=55 or sel['kalbakcellnd'][i]<10 or sel['kalfwdcellnd'][i]<5:
            s = encode_event(sel, i)
            if s not in cut_arr_mu:
                cut_arr_mu.append(s)

    return cut_arr_mu



def apply_cuts(Train_Params, cut_arr, file):
    """apply_cuts
    Function that the list of cut events and removes them from the mctruth events dictionary
    # Arguments
        Train_Params: the dictionary of events containing iscc and pdg data
        cut_arr: the list of events that did not pass the cuts
    # Returns
	Train_Params_Cut: the new dictionary of events
        events: the training data branch array index values of the events that passed the cut
        train: 2-dimential array, iscc, pdg
        train_val: 1-dimential array, iscc
    """
    Train_Params_Cut= copy.deepcopy(Train_Params)
    for i in cut_arr:
        if i in Train_Params_Cut.keys():
            del Train_Params_Cut[i]

    dataframes = []
    for key in Train_Params_Cut.keys():
        iscc = Train_Params_Cut[key][1]
        pdg = Train_Params_Cut[key][2]
        if iscc == 0:
            interaction = 1
        elif iscc == 1 and pdg*pdg ==144:
            interaction = 2
        elif iscc == 1 and pdg*pdg ==196:
            interaction = 3

        key_split = key.split(',')
        run = key_split[0][1:-1]
        subrun = key_split[1][1:-1]
        evt = key_split[2][1:-1]
        subevt = key_split[3][1:-1]
        cycle = key_split[4][1:-1]
        weight = key_split[5][1:-1]

        dataframes.append(pd.DataFrame({'run' : run,
                                        'subrun' : subrun,
                                        'evt' : evt,
                                        'subevt' : subevt,
                                        'cycle' : cycle,
                                        'weight': weight,
                                        'train_index' :  Train_Params_Cut[key][0],
                                        'label' : interaction,
                                        'file' : [file]
                                       }))

    df = pd.concat(dataframes)

    return df



def e_cuts(f, file):
    """e_cuts
    Function that takes in an hdf5 file and creates a list of events that do not pass the nu e cuts:
    nu mu cuts defined - "CAFAna/Cuts/NueCuts2017.h"
        cuts used - kNue2017NDFiducial && kNue2017NDContain && kNue2017NDFrontPlanes
    """
    ## Nu E Preselection Cuts  #
    cut_arr_e = []

    f = h5py.File(file, 'r')
    # kNue2017NDFiducial
    vtx_el = f[ 'rec.vtx.elastic']
    for i in range(len(vtx_el['vtx.x'])):
        a= vtx_el['vtx.x'][i]
        b= vtx_el['vtx.y'][i]
        c= vtx_el['vtx.z'][i]
        if a<=-100 or a>=160 or b<=-160 or b>=100 or c<=150 or c>=900:
            s = encode_event(vtx_el, i)
            if s not in cut_arr_e:
                cut_arr_e.append(s)

    # kNue2017NDContain
    shwlid = f['rec.vtx.elastic.fuzzyk.png.shwlid']
    for i in range(len(f['rec.vtx.elastic.fuzzyk.png.shwlid']['start.x'])):
        a = min(shwlid['start.x'][i],shwlid['stop.x'][i])
        b= max(shwlid['start.x'][i],shwlid['stop.x'][i])
        c= min(shwlid['start.y'][i], shwlid['stop.y'][i])
        d= max(shwlid['start.y'][i], shwlid['stop.y'][i])
        e = min(shwlid['start.z'][i], shwlid['stop.z'][i])
        f= max(shwlid['start.z'][i], shwlid['stop.z'][i])
        if a <=-170 or b >=170 or c <=-170 or d >=170 or e <=100 or f >=1225:
            s = encode_event(shwlid, i)
            if s not in cut_arr_e:
                cut_arr_e.append(s)
        pass

    # kNue2017NDFrontPlanes
    f = h5py.File(file, 'r')
    sel = f['rec.sel.contain']
    for i in range(len(sel['nplanestofront'])):
        if sel['nplanestofront'][i]<=6:
            s = encode_event(sel, i)
            if s not in cut_arr_e:
                cut_arr_e.append(s)

    return cut_arr_e



def maps(file):
    """maps Function that returns event maps from the hdf5 file
    # Arguments, file: hdf5 file name
    # Returns, maps - the arrays containing the image data
    """
    #import file
    f = h5py.File(file, 'r')

    # cvn training data branch
    cvnmaps = f['rec.training.cvnmaps']
    maps = cvnmaps['cvnmap']

    return maps



def image(maps):
    """image
    Function that the array containing the images and formats them correctly
    # Arguments
        maps: the array containing the images
    # Returns
	image_dataset: nested array containing two images for each event for the z-y and z-x planes
    """
    image_dataset = np.zeros(shape=(2,80,100))
    image_dataset[0,:,:] = np.rot90(np.asarray(maps[:8000]).reshape(100,80))
    image_dataset[1,:,:] = np.rot90(np.asarray(maps[8000:]).reshape(100,80))

    return image_dataset



def event(data, label):
    """event
    Function that takes in an array of event images and displays them
    # Arguments
        data: image array
        i: index within the array
    # Returns
	plot of image along with associated data
    """
    #first view z-x plane
    raw_im_1_cc = data[0,:,:]
    #second view z-y plane
    raw_im_2_cc = data[1,:,:]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 9))

    #axis lables
    ax1.imshow(raw_im_1_cc, aspect='auto')
    ax1.set_xlabel('z (units)')
    ax1.set_ylabel('x (units)')
    ax2.imshow(raw_im_2_cc, aspect='auto')
    ax2.set_xlabel('z (units)')
    ax2.set_ylabel('y (units)')
    fig.suptitle(label)



def open_df(path, num):
    """open_df
    Function that takes in a folder path and a number and unpickles the assosiated dataframe
    # Arguments
        path: path to the folder containing pickled dataframes
        num: dataframe number, e.g. df_1.pkl
    # Returns
	the dataframe containing the associated data
    """
    with open(path+ 'df_{}.pkl'.format(num),'rb') as file:
        df = pkl.load(file)
    return df



def open_df_gibuu(path, num):
    """open_df_gibuu
    Function that takes in a folder path and a number and unpickles the assosiated dataframe
    # Arguments
        path: path to the folder containing pickled dataframes
        num: dataframe number, e.g. df_gibuu_1.pkl
    # Returns
	the dataframe containing the associated data
    """
    with open(path+ 'df_gibuu_{}.pkl'.format(num), 'rb') as file:
        try:
            df = pkl.load(file)
            return df
        #some gibuu file dataframes were not correctly saved, leading to Unpickling Errors
        except pkl.UnpicklingError:
            print('Unpickling Error with gibuu df file {}'.format(num))
            pass


def generator(batch_size, steps_per_epoch, dataset, model = 'default'):
    """generator
    Generator function that yeilds a tensor contraining image data and the associated lables to be called by the keras 
    in training of the model
    # Arguments
        batch_size: path to the folder containing pickled dataframes
        steps_per_epoch: dataframe number, e.g. df_gibuu_1.pkl
        dataset: 
        model: 
    # Returns
	batch_images:
	batch_labels:
	batch_events: 
    """
    batch_images = np.zeros((batch_size, 2, 80, 100))
    batch_labels = np.zeros((batch_size, 3))
    batch_events = np.zeros((batch_size, 2))

    while True:
        dataset = dataset.sample(frac=1).reset_index(drop=True)

        for index in range(len(dataset['file'])):
            row = dataset.loc[index]

            images = image(maps(row['file'])[row['train_index']])
            images = (images - np.min(images))/ (np.max(images) - np.min(images))
            images.astype(float)

            if abs(row['label']- 1) <=10**(-5):
                labels = [1, 0, 0]
            elif abs(row['label']- 2) <=10**(-5):
                labels = [0, 1, 0]
            elif abs(row['label']- 3) <=10**(-5):
                labels = [0, 0, 1]

            if '_genie_' in str(row['file']):
                events = [1, 0]
            elif 'gibuu' in str(row['file']):
                events = [0, 1]

            batch_images[index%batch_size] = images
            batch_labels[index%batch_size] = labels
            batch_events[index%batch_size] = events

            if index%batch_size==0 and index!=0:
                if model== 'default':
                    yield batch_images, batch_labels
                elif model== 'descr':
                    yield batch_images, batch_events
                elif model== 'dann':
                    yield batch_images, [batch_labels, batch_events]
          



def test_generator(batch_size, steps_per_epoch, dataset, data='both', model_type = 'default'):

    for index in range(len(dataset['file'])):
        row = dataset.loc[index]
        weight = float(row['weight'])
        file = str(row['file'])
    
        rand = 0
        rand2, rand3 = 1, 1

        if (weight<= rand and 'gibuu' in file) or \
            (rand2 <= 0.98 and abs(float(dataset.loc[index]['label'])-3)<=10**(-5)) or \
            (rand3 <= 0.82 and abs(float(dataset.loc[index]['label'])-1)<=10**(-5)):
            pass
        
        else:
            images = image(maps(row['file'])[row['train_index']])
            images = (images - np.min(images))/ (np.max(images) - np.min(images))

            if '_genie_' in file:
                 mc = [1, 0]
            elif 'gibuu' in file:
                 mc = [0, 1]

            if abs(row['label']- 1) <=10**(-5):
                label = [1, 0, 0]
            elif abs(row['label']- 2) <=10**(-5):
                label = [0, 1, 0]
            elif abs(row['label']- 3) <=10**(-5):
                label = [0, 0, 1]

            yield images, row

			

def index_finder(probabilities, df_row):
	
    genie_index_below = []
    genie_index_above = []
    gibuu_index_below = []
    gibuu_index_above = []
	
    for i , prob in enumerate(probabilities):
        genie = prob[0][0]
	gibuu = prob[0][1]
        if gibuu<0.2:
            gibuu_index_below.append(i)
	elif gibuu>0.4 and gibuu<0.7:
	    gibuu_index_above.append(i)
	if genie<0.2:
            genie_index_below.append(i)
	elif genie>0.4 and gibuu<0.7:
	    genie_index_above.append(i)
		
    df_genie_below = pd.DataFrame(columns = df_row.columns)
    df_genie_above = pd.DataFrame(columns = df_row.columns)
    df_gibuu_below = pd.DataFrame(columns = df_row.columns) 
    df_gibuu_above = pd.DataFrame(columns = df_row.columns)
	
    for i, j in enumerate(genie_index_below):
        df_genie_below.loc[i] = df_row.loc[j]
	df_genie_below['type'] = ['genie below']*len(df_genie_below)
	
    for i, j in enumerate(genie_index_above):
        df_genie_above.loc[i] = df_row.loc[j]
	df_genie_above['type'] = ['genie above']*len(df_genie_above)
	
    for i, j in enumerate(gibuu_index_below):
        df_gibuu_below.loc[i] = df_row.loc[j]
	df_gibuu_below['type'] = ['gibuu below']*len(df_gibuu_below)
	
    for i, j in enumerate(gibuu_index_above):
        df_gibuu_above.loc[i] = df_row.loc[j]
	df_gibuu_above['type'] = ['gibuu above']*len(df_gibuu_above)
	
    frames = [df_genie_below, df_genie_above, df_gibuu_below, df_gibuu_above]
    df = pd.concat(frames)
    df.index = range(len(df['file']))
	
    for index, evt in enumerate(df['evt']):
        file = df['file'][index]
        f = h5py.File(file ,'r')
        mc = f['rec.mc.nu']
        columns = [str(i) for i in mc.keys()]
        df_out = pd.DataFrame(columns = columns)

        for count, val in enumerate(mc['evt']):
            if int(val) == int(evt):
                if int(mc['subevt'][count]) ==  int(df['subevt'][index]):
                    mc_index = count
                    break
            elif int(val)>=int(evt)+1:
                break
        row = [mc[str(i)][mc_index][0] for i in mc.keys()]
        df_out.loc[index] = row

    return df_out

\end{lstlisting}

\section*{methods.py}

\begin{lstlisting}[language=Python]
from functions import *
from mobilenetv2 import *
from keras.models import Model
from itertools import islice


def cuts(path):
    """cuts
    Function that takes in an hdf5 file and applied multiple functions to :
    nu mu cuts defined - "CAFAna/Cuts/NueCuts2017.h"
        cuts used - kNue2017NDFiducial && kNue2017NDContain && kNue2017NDFrontPlanes
    """
    #import files
    file_dir = get_files(path)

    #split input files into batches of 50 files

    batches = [file_dir[i:i+100] for i in range(0, len(file_dir), 100)]
    batch_no = 0

    #for set of 50 files
    for i in batches:
        batch_no+=1

        dataframes = []

        #for file in batch
        for j in i:

            try :
                hdf, file, Train_Params = data(path + '/' + j)

                ######### Cuts #############################################################

                cut_arr_mu = mu_cuts(hdf, file)
                dataframes.append( apply_cuts(Train_Params, cut_arr_mu, file) )

                cut_arr_e = e_cuts(hdf, file)
                dataframes.append( apply_cuts(Train_Params, cut_arr_e, file))

            except OSError:
                pass

        #### Save files #########################################################################

        out_path = '/home/ishokar/dataframes/'

        df = pd.concat(dataframes)
        df.index = range(len(df['file']))

        with open(out_path + '/df_{}.pkl'.format(batch_no),'wb') as f1:
            pkl.dump(df, f1)







def train(train_type = 'default',
          epochs= 200,
          batch_size = 32,
          dataset_percent = 0.8,
          call_back_patience = 10,
          learning_rate = 0.001,
          DANN_strength = 0.1, 
          model_optimiser='SGD',
          out_file_name = '32_SGD'):

    """train_mobnetmodelv2
    Function that compiles and trains the mobilenet network
    # Arguments/Hyperparamaters
        epochs: number of epochs,
        batch_size : batch size,
        learning_rate : learning rate,
        call_back_patience : number of epochs patience before stopping training if no improvement takes place,
        save_best : saves weights of the best model based on validation loss
    # Returns
	mobnetmodel: trained model
        history : training statistics
    """
    path = '/home/ishokar/dataframes/'
    df_train = open_df(path, 'train_equal')
    df_train.index = range(len(df_train['file']))
    steps_per_epoch = round(len(df_train['file'])/(batch_size)*dataset_percent)
    
    df_val = open_df(path, 'val_equal')
    df_val.index = range(len(df_val['file']))
    val_steps_per_epoch = round(len(df_val['file'])/(batch_size)*dataset_percent)

    if model_optimiser == 'SGD':
        opt= SGD(lr=learning_rate, momentum=0.9)
    elif model_optimiser == 'Adam':
        opt = Adam(learning_rate=learning_rate)
    else:
        print('Not valid model_optimiser name')

    callbacks = [EarlyStopping(monitor='accuracy', patience=call_back_patience)]

    if train_type == 'dann':
        mobnetmodel = MobileNetV2_DANN(input_shape=((2, 80, 100),), classifier_classes=3, descriminator_classes = 2, DANN_strength = DANN_strength)
        mobnetmodel.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])

    elif train_type== 'default':
        mobnetmodel = mobilenetv2.MobileNetV2(input_shape=((2, 80, 100),), classes=3)
        mobnetmodel.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])

    elif train_type == 'descr':
        mobnetmodel = mobilenetv2.MobileNetV2(input_shape=((2, 80, 100),), classes=2)
        mobnetmodel.compile(optimizer=opt, loss='binary_crossentropy',metrics=['accuracy'])

    history = mobnetmodel.fit_generator(generator=generator(batch_size, steps_per_epoch, df_train, model = train_type),
                                        steps_per_epoch= steps_per_epoch,
                                        validation_data= generator(batch_size, val_steps_per_epoch, df_val, model = train_type),
                                        validation_steps= val_steps_per_epoch,
                                        epochs=epochs,
                                        callbacks = callbacks)

    mobnetmodel.save_weights("weights_{}.h5".format(out_file_name))
    history.model = None
    pkl.dump(history, open( "history_{}.pkl".format(out_file_name), "wb" ))

    return mobnetmodel, history







def test(weights_file, path, name, dataset_percent = 0.1, data = 'both', model_type = 'default', output = 'default'):

    probabilities =[]
    layer_nodes = []    
    batch_no = 32

    df_test = open_df(path, 'test_equal')
    df_test.index = range(len(df_test['file']))
    df_test =df_test.sample(frac=1).reset_index(drop=True)
    steps_per_epoch = round(len(df_test['file'])*dataset_percent)
    
    columns = df_test.columns    
    df_row = pd.DataFrame(columns = columns)

    if model_type == 'default':
        model = MobileNetV2(input_shape=((2, 80, 100),), classes=3, )

    elif model_type == 'descr':
        model =MobileNetV2(input_shape=((2, 80, 100),), classes=2, )

    elif model_type == 'dann':
        model = MobileNetV2_DANN(input_shape=((2, 80, 100),), classifier_classes=3, descriminator_classes = 2)

    model.load_weights('/home/ishokar/march_test/output_weights' + weights_file)

    steps = 0
    for data, row_0 in islice(test_generator(1, 1, df_test, data, model_type), steps_per_epoch):
        df_row.loc[steps] = row_0

        data = data.reshape((1, 2, 80, 100))
        probabilities_0 = model.predict(data, steps = 1)
        probabilities.append(probabilities_0)

        intermediate_layer_model = Model(inputs=model.input,outputs=model.layers[-3].output)
        layer_nodes_0 = intermediate_layer_model.predict(data)
        layer_nodes.append(layer_nodes_0)
        
        if model_type == 'default':
            lab = df_row.loc[steps]['label']
        else:
            lab = df_row.loc[steps]['label']
        print(steps,'/', steps_per_epoch, ':', round((steps*100)/steps_per_epoch, 2), '%,', probabilities_0[0], lab) 
        steps+=1
    
    pkl.dump(layer_nodes, open('files_new/nodes_values_{}_{}.pkl'.format(name, weights_file[8:-3]),'wb'))
    pkl.dump(probabilities, open('files_new/test_probabilities_{}_{}.pkl'.format(name, weights_file[8:-3]),'wb'))
    pkl.dump(df_row, open('files_new/test_df_{}_{}.pkl'.format(name, weights_file[8:-3]),'wb'))
	
    df2 = index_finder(probabilities, df_row)
    pkl.dump(df2, open('df_physics.pkl','wb'))

\end{lstlisting}

\section*{cuts.py}

\begin{lstlisting}[language=Python]
from functions import *
from methods import *

#function that applies the cuts method to a folder containing hdf5 files and returns a dataframe
cuts('/unix/nova/hdf5/ND-ProngCVN-FHC')

\end{lstlisting}

\section*{mobilenetv2.py}

\begin{lstlisting}[language=Python]
from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

import tensorflow as tf
import keras
from keras import backend as K, optimizers

from keras.engine import Layer
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Reshape
from keras.layers import Activation
from keras.layers import BatchNormalization
from keras.layers import MaxPooling2D
from keras.layers import GlobalAveragePooling2D
from keras.layers import GlobalMaxPooling2D
from keras.layers import DepthwiseConv2D
from keras.layers import Conv2D
from keras.layers import Lambda
from keras.layers import ReLU
from keras.layers import GaussianDropout
from keras.layers import add
from keras.layers import concatenate
from keras.layers import multiply
from keras.regularizers import l2

from keras.backend import get_session

#############################################################################

def MobileNetV2(input_shape=None,
                re_shape=(2,100,80,1),
                alpha=0.25,
                depth_multiplier=1,
                classes=5,
                weightdecay=0.0002,
                jitter=0.001,
                input_tensor=None):

    """MobileNetv1
    This function defines a MobileNetv1 architectures.
    # Arguments
        inputs: Inuput Tensor, e.g. an image
        alpha: Width Multiplier
        depth_multiplier: Resolution Multiplier
        classes: number of labels
        weightdecay: weight decay for last layer
    # Returns
	five MobileNetv2 model stages."""
    if input_tensor is None:
        img_input = Input(shape=input_shape[0], name='input')
    inputs=img_input
    if jitter != 0:
        img_input_jitter = GaussianDropout(jitter)(img_input)
        shaped = Reshape(re_shape)(img_input_jitter)
    else:
        shaped = Reshape(re_shape)(img_input)
    def _lambda_unstack(x):
        import tensorflow as tf
        return tf.unstack(x,axis=1)
    shaped = Lambda(_lambda_unstack)(shaped)
    img_input1 = shaped[0]
    img_input2 = shaped[1]
    if(re_shape[0] == 4):
        img_input3 = shaped[2]
        img_input4 = shaped[3]
    if(re_shape[0] == 4):
        img_input = [img_input1, img_input2, img_input3, img_input4]
    else:
        img_input  = [img_input1, img_input2]

    branches = []
    names = ['x','y','px','py']
    for i in range(len(img_input)):
        branch = subnet(img_input[i], names[i], alpha)
        branches.append(branch)

    merge = concatenate(branches)

    merge = _inverted_residual_block(merge, 64,  (3, 3), t=6, strides=2, n=4, alpha=alpha, block_id=7, name='merge')
    merge = _inverted_residual_block(merge, 96,  (3, 3), t=6, strides=1, n=3, alpha=alpha, block_id=11, name='merge')
    merge = _inverted_residual_block(merge, 160, (3, 3), t=6, strides=2, n=3, alpha=alpha, block_id=14, name='merge')
    merge = _inverted_residual_block(merge, 320, (3, 3), t=6, strides=1, n=1, alpha=alpha, block_id=17, name='merge')
    merge = _conv_block(merge, 1280, alpha, (1, 1), strides=(1, 1), block_id=18, name='merge')

    merge = GlobalAveragePooling2D()(merge)
    merge = Dropout(0.4)(merge)
    merge = Dense(1024,activation='relu')(merge)
    merge = Dropout(0.4)(merge)

    out = Dense(classes,
                use_bias=False,
                kernel_regularizer=l2(weightdecay),
                activation='softmax',
                name='output')(merge)

    model = Model(inputs=inputs, outputs=out, name='mobilenetv2')
    print(model)
    # load weights
    return model

#############################################################################

def MobileNetV2_DANN(input_shape=None,
                    re_shape=(2,100,80,1),
                    DANN_strength = 0.1,
                    alpha=0.25,
                    depth_multiplier=1,
                    classifier_classes=3,
                    descriminator_classes=2,
                    batch_size = 32,
                    weightdecay=0.0002,
                    jitter=0.001,
                    input_tensor=None):

    """MobileNetv1
    This function defines a MobileNetv1 architectures.
    # Arguments
        inputs: Inuput Tensor, e.g. an image
        alpha: Width Multiplier
        depth_multiplier: Resolution Multiplier
        classes: number of labels
        weightdecay: weight decay for last layer
    # Returns
	five MobileNetv2 model stages."""
    if input_tensor is None:
        img_input = Input(shape=input_shape[0], name='input')
    inputs=img_input
    if jitter != 0:
        img_input_jitter = GaussianDropout(jitter)(img_input)
        shaped = Reshape(re_shape)(img_input_jitter)
    else:
        shaped = Reshape(re_shape)(img_input)
    def _lambda_unstack(x):
        import tensorflow as tf
        return tf.unstack(x,axis=1)
    shaped = Lambda(_lambda_unstack)(shaped)
    img_input1 = shaped[0]
    img_input2 = shaped[1]
    if(re_shape[0] == 4):
        img_input3 = shaped[2]
        img_input4 = shaped[3]
    if(re_shape[0] == 4):
        img_input = [img_input1, img_input2, img_input3, img_input4]
    else:
        img_input  = [img_input1, img_input2]

    branches = []
    names = ['x','y','px','py']
    for i in range(len(img_input)):
        branch = subnet(img_input[i], names[i], alpha)
        branches.append(branch)

    merge = concatenate(branches)

    merge = _inverted_residual_block(merge, 64,  (3, 3), t=6, strides=2, n=4, alpha=alpha, block_id=7, name='merge')
    merge = _inverted_residual_block(merge, 96,  (3, 3), t=6, strides=1, n=3, alpha=alpha, block_id=11, name='merge')
    merge = _inverted_residual_block(merge, 160, (3, 3), t=6, strides=2, n=3, alpha=alpha, block_id=14, name='merge')
    merge = _inverted_residual_block(merge, 320, (3, 3), t=6, strides=1, n=1, alpha=alpha, block_id=17, name='merge')
    merge = _conv_block(merge, 1280, alpha, (1, 1), strides=(1, 1), block_id=18, name='merge')

    av_pool = GlobalAveragePooling2D()(merge)

    merge = Dropout(0.4)(av_pool)
    merge = Dense(1024,activation='relu')(merge)
    merge = Dropout(0.4)(merge)

    #insert discriminator

    grl_layer = GradientReversal(1.0)
    feature_output_grl = grl_layer(av_pool)

    labeled_feature_output = Lambda(lambda x: K.switch(K.variable(1), K.concatenate([x[:int(batch_size//2)], x[:int(batch_size//2)]], axis=0), x), output_shape=lambda x: x[0:])(feature_output_grl)

    out = Dropout(0.5)(labeled_feature_output)
    out = Dense(128, activation="relu")(out)
    out = Dropout(0.5)(out)
    discriminator_output = Dense(descriminator_classes, activation="softmax", name="discriminator_output")(out)

    classifier_output = Dense(classifier_classes,
                              use_bias=False,
                              kernel_regularizer=l2(weightdecay),
                              activation='softmax',
                              name='output')(merge)


    model = Model(inputs=inputs, outputs=[classifier_output, discriminator_output], name='mobilenetv2')
    print(model)

    return model


### GRADIENT REVERSAL BLOCK ######################################################################

def reverse_gradient(X, hp_lambda):
    '''Flips the sign of the incoming gradient during training.'''
    try:
        reverse_gradient.num_calls += 1
    except AttributeError:
        reverse_gradient.num_calls = 1

    grad_name = "GradientReversal%d" % reverse_gradient.num_calls

    @tf.RegisterGradient(grad_name)
    def _flip_gradients(op, grad):
        return [tf.negative(grad) *0.5* hp_lambda]

    g = get_session().graph
    with g.gradient_override_map({'Identity': grad_name}):
        y = tf.identity(X)

    return y

class GradientReversal(Layer):
    '''Flip the sign of gradient during training.'''
    def __init__(self, hp_lambda, **kwargs):
        super(GradientReversal, self).__init__(**kwargs)
        self.supports_masking = False
        self.hp_lambda = hp_lambda

    def build(self, input_shape):
        self.trainable_weights = []

    def call(self, x, mask=None):
        return reverse_gradient(x, self.hp_lambda)

    def get_output_shape_for(self, input_shape):
        return input_shape

    def get_config(self):
        config = {}
        base_config = super(GradientReversal, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


##############################################################################################

def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1), block_id=1, name=''):
    """Adds an initial convolution layer (with batch normalization and relu6).
    # Arguments
        inputs: Input tensor of shape `(rows, cols, chans)`
            (with `channels_last` data format) or
            (chans, rows, cols) (with `channels_first` data format).
            It should have exactly 3 inputs channels,
            and width and height should be no smaller than 32.
            E.g. `(224, 224, 3)` would be one valid value.
        filters: Integer, the dimensionality of the output space
            (i.e. the number output of filters in the convolution).
        alpha: controls the width of the network.
            - If `alpha` < 1.0, proportionally decreases the number
                of filters in each layer.
            - If `alpha` > 1.0, proportionally increases the number
                of filters in each layer.
            - If `alpha` = 1, default number of filters from the paper
                 are used at each layer.
        kernel: An integer or tuple/list of 2 integers, specifying the
            width and height of the 2D convolution window.
            Can be a single integer to specify the same value for
            all spatial dimensions.
        strides: An integer or tuple/list of 2 integers,
            specifying the strides of the convolution along the width and height.
            Can be a single integer to specify the same value for
            all spatial dimensions.
            Specifying any stride value != 1 is incompatible with specifying
            any `dilation_rate` value != 1.
    # Input shape
        4D tensor with shape:
        `(samples, channels, rows, cols)` if data_format='channels_first'
        or 4D tensor with shape:
        `(samples, rows, cols, channels)` if data_format='channels_last'.
    # Output shape
        4D tensor with shape:
        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'
        or 4D tensor with shape:
        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.
        `rows` and `cols` values might have changed due to stride.
    # Returns
	Output tensor of block.
    """
    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
    filters = int(filters * alpha)
    x = Conv2D(filters, kernel,
               padding='same',
               use_bias=False,
               strides=strides,
               name=name+'conv{}'.format(block_id))(inputs)
    x = BatchNormalization(axis=channel_axis, name=name+'conv{}_bn'.format(block_id))(x)
    return ReLU(6, name=name+'conv{}_relu'.format(block_id))(x)

def _bottleneck(inputs, filters, kernel, t, s, r=False, alpha=1.0, block_id=1, train_bn = False, name=''):
    """Bottleneck
    This function defines a basic bottleneck structure.
    # Arguments
        inputs: Tensor, input tensor of conv layer.
        filters: Integer, the dimensionality of the output space.
        kernel: An integer or tuple/list of 2 integers, specifying the
            width and height of the 2D convolution window.
        t: Integer, expansion factor.
            t is always applied to the input size.
        s: An integer or tuple/list of 2 integers,specifying the strides
            of the convolution along the width and height.Can be a single
            integer to specify the same value for all spatial dimensions.
        r: Boolean, Whether to use the residuals.
    # Returns
	Output tensor.
    """
    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
    tchannel = K.int_shape(inputs)[channel_axis] * t
    filters = int(alpha * filters)
    x = _conv_block(inputs, tchannel, alpha, (1, 1), (1, 1),block_id=block_id, name=name)
    x = DepthwiseConv2D(kernel,
                    strides=(s, s),
                    depth_multiplier=1,
                    padding='same',
                    name=name+'conv_dw_{}'.format(block_id))(x)
    x = BatchNormalization(axis=channel_axis,name=name+'conv_dw_{}_bn'.format(block_id))(x)
    x = ReLU(6, name=name+'conv_dw_{}_relu'.format(block_id))(x)
    x = Conv2D(filters,
                    (1, 1),
                    strides=(1, 1),
                    padding='same',
                    name=name+'conv_pw_{}'.format(block_id))(x)
    x = BatchNormalization(axis=channel_axis, name=name+'conv_pw_{}_bn'.format(block_id))(x, training=train_bn)
    if r:
        x = add([x, inputs], name=name+'res{}'.format(block_id))
    return x

def _inverted_residual_block(inputs, filters, kernel, t, strides, n, alpha, block_id, name=''):
    """Inverted Residual Block
    This function defines a sequence of 1 or more identical layers.
    # Arguments
        inputs: Tensor, input tensor of conv layer.
        filters: Integer, the dimensionality of the output space.
        kernel: An integer or tuple/list of 2 integers, specifying the
            width and height of the 2D convolution window.
        t: Integer, expansion factor.
            t is always applied to the input size.
        s: An integer or tuple/list of 2 integers,specifying the strides
            of the convolution along the width and height.Can be a single
            integer to specify the same value for all spatial dimensions.
        n: Integer, layer repeat times.
    # Returns
	Output tensor.
    """
    x = _bottleneck(inputs, filters, kernel, t, strides, False, alpha, block_id, name=name)
    for i in range(1, n):
        block_id += 1
        x = _bottleneck(x, filters, kernel, t, 1, True, alpha, block_id, name=name)
    return x

def subnet(x, name, alpha):
    x = _conv_block(x, 32, alpha, (3, 3), strides=(2, 2), block_id=0, name=name)
    x = _inverted_residual_block(x, 16,  (3, 3), t=1, strides=1, n=1, alpha=alpha, block_id=1, name=name)
    x = _inverted_residual_block(x, 24,  (3, 3), t=6, strides=2, n=2, alpha=alpha, block_id=2, name=name)
    x = _inverted_residual_block(x, 32,  (3, 3), t=6, strides=2, n=3, alpha=alpha, block_id=4, name=name)

    return x

\end{lstlisting}

\section*{train.py}

\begin{lstlisting}[language=Python]
from methods import *

# model_optimiser options: 'Adam', 'SGD'

train(train_type = 'dann',
      epochs= 100,
      batch_size = 32,
      dataset_percent = 0.9,
      call_back_patience = 10,
      learning_rate = 0.001,
      DANN_strength = 0.1,
      model_optimiser='Adam',
      out_file_name = '32_Adam_dann_0.5_02_03')

\end{lstlisting}

\section*{test.py}

\begin{lstlisting}[language=Python]
from methods import *

#function paramemters

path = '/home/ishokar/dataframes/'

data = 'both'
model_type = 'descr'
output = 'default'

file= '/weights_train_100_descr_32_sgd_22_02.h5'
file_name = 'default_TOboth_equal_1'

dataset_percent = 0.001

test(file, path, file_name, dataset_percent, data, model_type, output)

\end{lstlisting}

\section*{analysis.py}

\begin{lstlisting}[language=Python]
from functions import *
from methods import *
import seaborn as sns
import matplotlib.pyplot as plt
import pickle as pkl
import matplotlib.pyplot as plt
from sklearn import metrics

def plot_history(path, file):
    
    with open(path + file,'rb') as f1:
        history = pkl.load(f1)
        
    fig = plt.figure(figsize=(16,8))

    ax1 = fig.add_axes([0, 0, 1, 1])
    ax2 = fig.add_axes()
    ax2 = ax1.twinx() 

    lns1 = ax1.plot(history.history['loss'][:100], color='red', label='loss')
    lns2 = ax1.plot(history.history['val_loss'][:100], color='green', label='val_loss')

    lns3 = ax2.plot(history.history['accuracy'][:100], color='blue', label='accuracy')
    lns4 = ax2.plot(history.history['val_accuracy'][:100], color='orange', label='val_accuracy')

    leg = lns1 + lns2 + lns3 + lns4
    labs = [l.get_label() for l in leg]
    ax1.legend(leg, labs, loc='upper left')
    plt.title('', fontsize=20)

    plt.rcParams.update({'font.size': 14})

    ax1.set_ylim(0.0, 5)
    ax1.set_ylabel('loss')

    ax2.set_ylim(0.1, 0.85)
    ax2.set_ylabel('accuracy')

    ax1.set_xlabel('epochs')

    plt.show()
    
    

def test_results(path, model_name):
    
    with open(path+ 'test_probabilities__{.format(model_name)}.pkl'.format(model_name),'rb') as f1:
        probabilities = pkl.load(f1)
    with open(path+ 'test_df__{}.pkl'.format(model_name),'rb') as f2:
        df = pkl.load(f2)
    with open(path+ 'df_physics_{}.pkl.format(model_name)','rb') as f3:
        physics_df = pkl.load(f3)
    with open(path+ 'nodes_values_default_{}.pkl'.format(model_name),'rb') as f4:
    node_values = pkl.load(f4)
    
    return probabilities, df, physics_df, node_values



def unpack_df():
    labels = list(df['label'])
    gibuu_weights = list(df['weight'])

    events = np.zeros((len(df['file']), 2))
    
    for i in range(len(df['file'])):
        if '_genie_' in str(df['file'][i]):
            events[i] = [1, 0]
        elif 'gibuu' in str(df['file'][i]):
            events[i] = [0, 1]
            
    def sub(x):
        return x-1
    
    test_vals = list(map(sub, labels))   
    
    return labels, gibuu_weights, events, test_vals



def predictions():
    
    predictions = []
    for i in probabilities:
        nc = i[0][0]
        nu_e = i[0][1]
        nu_mu = i[0][2]
        if nc>= nu_e and nc>=nu_mu:
            predictions.append(0)
        elif nu_e>= nc and nu_e>=nu_mu:
            predictions.append(1)
        elif nu_mu>= nu_e and nu_mu>=nc:
            predictions.append(2)

    #accuracy
    acc = 0
    for i in range(len(probabilities)):
        if test_vals[i]==predictions[i]:
            acc+=1
        else:
            pass
    acc/=len(test_vals)
    
    
    print('Accuracy:{} \n'.format(acc))

    #printing first 10 events to check data
    
    print('Probabilities: \n')
    for i in range(10):
        print(probabilities[i], '\n')
    print('Predictions: \n')
    print(predictions[:10], '\n')
    print('Truth labels: \n')
    print(test_vals[:10])
    
    return predictions



def event_hist(data, data_type):
    # data_type options: test_vals, predictions
    
    plt.figure(figsize=(12,6))
    plt.hist(data)
    x = [0.1, 1.1, 1.9]
    class_names = ['nc', 'nu_e', 'nu_mu']
    plt.xticks(x, class_names)
    plt.ylabel('Count')
    if data_type== 'test_vals':
        plt.title('MC Truth Classes')
    elif data_type== 'predictions':
        plt.title('Predicited Classes')
    
    
        
def classifier_output(probabilities, interaction) :      

    if interaction == 'nc':
        index = 0
    elif interaction == 'nu_e':
        index = 1
    elif interaction == 'nu_mu':
        index = 2
        
    mu_e = []
    nc = []
    nu_mu = []
    
    for i in range(len(probabilities)):
        if test_vals[i] ==0 :
            nc.append(probabilities[i][0][index])
        elif test_vals[i] ==1:
            mu_e.append(probabilities[i][0][index])
        elif test_vals[i] ==2:
            nu_mu.append(probabilities[i][0][index])

    plt.figure(figsize=(25,10))
    factor = 1/(len(test_vals))
    
    (counts, bins) = np.histogram(nc, bins=100)
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, linestyle=('solid'),color=('b'))
    
    (counts, bins) = np.histogram(mu_e, bins=100)
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, linestyle=('solid'),color=('orange'))
    
    (counts, bins) = np.histogram(nu_mu, bins=100)
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, linestyle=('solid'),color=('g'))

    plt.legend(['nc', 'nu_e','nu_mu'], loc='upper left')
    plt.ylabel('Percentage of test events')
    plt.xlim(0,1)
    plt.xlabel('{} classifer Output'.format(interaction))
    
    
    

def purity_efficiency(probabilites, interaction):
    
    if interaction == 'nc':
        index = 0
    elif interaction == 'nu_e':
        index = 1
    elif interaction == 'nu_mu':
        index = 2
    
    purity_list = []
    efficiency_list = []
    p_x_e_list = []

    for j in np.arange(0, 0.99, 0.01):
        nu_mu_above = []
        nu_mu_below = []
        nc_above = []
        nc_below = []
        nu_e_above = []
        nu_e_below = []
        

        for i in range(len(probabilities)):
            if test_vals[i] ==2:
                if probabilities[i][0][index]>=j:
                    nu_mu_above.append(probabilities[i][0][index]*gibuu_weights[i])
                elif probabilities[i][0][index]<=j:
                    nu_mu_below.append(probabilities[i][0][index]*gibuu_weights[i])

            elif test_vals[i] ==0:         
                if probabilities[i][0][index]>=j:
                    nc_above.append(probabilities[i][0][index]*gibuu_weights[i])
                elif probabilities[i][0][index]<=j:
                    nc_below.append(probabilities[i][0][index]*gibuu_weights[i])

            elif test_vals[i] ==1:        
                if probabilities[i][0][index]>=j:
                    nu_e_above.append(probabilities[i][0][index]*gibuu_weights[i])
                elif probabilities[i][0][index]<=j:
                    nu_e_below.append(probabilities[i][0][index]*gibuu_weights[i])
        
        if interaction == 'nc':
            purity = len(nc_above)/(len(nc_above)+len(nu_mu_above)+len(nu_e_above))
            efficiency = len(nc_above)/(len(nc_above)+len(nc_below))
            
        elif interaction == 'nu_e':
            purity = len(nu_e_above)/(len(nc_above)+len(nu_mu_above)+len(nu_e_above))
            efficiency = len(nu_e_above)/(len(nu_e_above)+len(nu_e_below))
            
        elif interaction == 'nu_mu':      
            purity = len(nu_mu_above)/(len(nc_above)+len(nu_mu_above)+len(nu_e_above))
            efficiency = len(nu_mu_above)/(len(nu_mu_above)+len(nu_mu_below))
            
        purity_list.append(purity*100)
        efficiency_list.append(efficiency*100)
        p_x_e_list.append(purity*efficiency*100)
    
    plt.figure(figsize=(25,10))
    plt.plot(purity_list)
    plt.plot(efficiency_list)
    plt.plot(p_x_e_list)
    plt.xlabel('{} classifer Output Percentage'.format(interaction))
    plt.ylabel('Percentage')
    plt.legend(['Purity', 'Efficiency', 'Purity* Efficiency'], loc='lower left')
    
    
    
    
def roc(probabilities, test_vals):
    pr_nc = []
    pr_nu_e = []
    pr_nu_mu = []
    for i in range(len(probabilities)):
        pr_nc.append(probabilities[i][0][0])
        pr_nu_e.append(probabilities[i][0][1])
        pr_nu_mu.append(probabilities[i][0][2])

    nc_fpr, nc_tpr, nc_thresholds = metrics.roc_curve(test_vals, pr_nc, pos_label=0)
    nu_e_fpr, nu_e_tpr, nu_e_thresholds = metrics.roc_curve(test_vals, pr_nu_e, pos_label=1)
    nu_mu_fpr, nu_mu_tpr, nu_mu_thresholds = metrics.roc_curve(test_vals, pr_nu_mu, pos_label=2)

    plt.figure(figsize=(10,10))
    plt.plot(nc_fpr, nc_tpr, label = 'NC')
    plt.plot(nu_e_fpr, nu_e_tpr, label = 'Nu E')
    plt.plot(nu_mu_fpr, nu_mu_tpr, label = 'Nu Mu')
    plt.legend(['NC', 'Nu E', 'Nu Mu',], loc='upper left')
    plt.xlabel('False positive rate')
    plt.ylabel('True positive rate')
    plt.title('ROC curve')
    plt.plot([0,1], [0,1], '--')
    
    
    
def node_event(node):
    bins = 20
    nc_genie = []
    nc_gibuu = []
    nu_e_genie = []
    nu_e_gibuu = []
    nu_mu_genie = []
    nu_mu_gibuu = []
    for i in range(len(events)):
        if test_vals[i] == 0 and events[i][0] == 1:
            nc_genie.append(node_values[i][0][node])
        elif test_vals[i] == 0 and events[i][1]  == 1:
            nc_gibuu.append(node_values[i][0][node])
        elif test_vals[i] == 1 and events[i][0]  == 1:
            nu_e_genie.append(node_values[i][0][node])
        elif test_vals[i] == 1 and events[i][1]  == 1:
            nu_e_gibuu.append(node_values[i][0][node])
        elif test_vals[i] == 2 and events[i][0]  == 1:
            nu_mu_genie.append(node_values[i][0][node])
        elif test_vals[i] == 2 and events[i][1]  == 1:
            nu_mu_gibuu.append(node_values[i][0][node])

    dataset_ = [nc_genie, nc_gibuu, nu_e_genie, nu_e_gibuu, nu_mu_genie, nu_mu_gibuu]
    label = ['nc_genie', 'nc_gibuu', 'nu_e_genie', 'nu_e_gibuu', 'nu_mu_genie', 'nu_mu_gibuu']

    plt.figure(figsize=(8,5))
    (counts, bins) = np.histogram(nc_genie, bins=bins)
    factor = 1/(len(nc_genie))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nc_genie', linestyle=('solid'),color=('g'))
    
    (counts, bins) = np.histogram(nc_gibuu, bins=bins)
    factor = 1/(len(nc_gibuu))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nc_gibuu', linestyle=('dashed'),color=('g'))
    
    (counts, bins) = np.histogram(nu_e_genie, bins=bins)
    factor = 1/(len(nu_e_genie))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nu_e_genie', linestyle=('solid'),color=('r'))
    
    (counts, bins) = np.histogram(nu_e_gibuu, bins=bins)
    factor = 1/(len(nu_e_gibuu))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nu_e_gibuu', linestyle=('dashed'),color=('r'))
    
    (counts, bins) = np.histogram(nu_mu_genie, bins=bins)
    factor = 1/(len(nu_mu_genie))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nu_mu_genie', linestyle=('solid'),color=('b'))
    
    (counts, bins) = np.histogram(nu_mu_gibuu, bins=bins)
    factor = 1/(len(nu_mu_gibuu))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='nu_mu_gibuu', linestyle=('dashed'),color=('b'))

    plt.title('Node Number {}'.format(node))
    plt.legend(prop={'size': 10})
    plt.xlabel('Node Value')
    plt.ylabel('Percentage of Data')
    plt.xlim(0.05,1)
    plt.ylim(0,0.2)
    
    
    
def node_pe(node):
    for j in np.arange(0, 0.25, 0.01):
        purity_list = []
        efficiency_list = []
        p_x_e_list = []
        
        nu_mu_above = []
        nu_mu_below = []
        nc_above = []
        nc_below = []
        nu_e_above = []
        nu_e_below = []
        for i in range(len(probabilities)):
            if test_vals[i] ==2:
                if probabilities[i][0][1]>=j:
                    nu_mu_above.append(node_values[i][0][node])
                elif probabilities[i][0][1]<=j:
                    nu_mu_below.append(node_values[i][0][node])

            elif test_vals[i] ==0:         
                if probabilities[i][0][1]>=j:
                    nc_above.append(node_values[i][0][node])
                elif probabilities[i][0][1]<=j:
                    nc_below.append(node_values[i][0][node])

            elif test_vals[i] ==1:        
                if probabilities[i][0][1]>=j:
                    nu_e_above.append(node_values[i][0][node])
                elif probabilities[i][0][1]<=j:
                    nu_e_below.append(node_values[i][0][node])
                    
                    
    purity = len(nu_e_above)/(len(nc_above)+len(nu_mu_above)+len(nu_e_above))
    purity_list.append(purity)

    efficiency = len(nu_e_above)/(len(nu_e_above)+len(nu_e_below))
    efficiency_list.append(efficiency)

    p_x_e_list.append(purity*efficiency)

    fig = plt.figure(figsize=(20,10))
    plt.plot(purity_list)
    plt.plot(efficiency_list)
    plt.plot(p_x_e_list)
    plt.xlabel('Nu Mu classifer Output')
    plt.ylabel('Percentage')
    plt.title('Trained and Tested on Both Datasets')
    plt.legend(['Purity', 'Efficiency', 'Purity* Efficiency'], loc='lower left')
    
def domain_physics(text, bins):

    plt.figure(figsize=(18,10))
    (counts, bins) = np.histogram(df2[text], bins=bins)
    factor = 1/(len(df2[text]))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='Gibuu<0.2')

    (counts, bins) = np.histogram(df3[text], bins=bins)
    factor = 1/(len(df3[text]))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label= 'Genie<0.2')

    (counts, bins) = np.histogram(df4[text], bins=bins)
    factor = 1/(len(df4[text]))
    plt.hist(bins[:-1], bins, weights=factor*counts, histtype='step', fill=False, label='0.4<Gibuu<0.7')
    
    plt.legend(prop={'size': 14})
    plt.ylabel('Percentage of Data')
    plt.xlabel(text)
    plt.show()

\end{lstlisting}